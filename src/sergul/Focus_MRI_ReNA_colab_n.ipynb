{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ajoshiusc/lesion-detector/blob/master/src/sergul/Focus_MRI_ReNA_colab_n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NujF1WdFUdYm"
   },
   "source": [
    "# MRI - packages\n",
    "Alhussain Almarhabi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "KDrYwra2TYlp",
    "outputId": "99c43387-069f-46df-ed90-d655423c5727"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "3G8dmxLrCqpO",
    "outputId": "2d185cd3-9609-4e2a-e91e-0ba3a4df934e"
   },
   "outputs": [],
   "source": [
    "#!rm -rf MRI_ReNA\n",
    "# clone my github repository\n",
    "#!git clone https://github.com/code-Eng/MRI_ReNA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ihitDjI-Cn8L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Load the Drive helper and mount\\n#from google.colab import drive\\n\\n# This will prompt for authorization.\\n#drive.mount('/content/drive')\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "#drive.mount('/content/drive')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcUH44W9Yf3n"
   },
   "source": [
    "#rena.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Pw5H7JsJYkjU",
    "outputId": "395dafd7-f2bf-46be-aa99-6ef66cec8709"
   },
   "outputs": [],
   "source": [
    "#%%writefile rena.py\n",
    "\n",
    "\"\"\"Recursive nearest agglomeration (ReNA):\n",
    "    fastclustering for approximation of structured signals\n",
    "Author:\n",
    "    Andres Hoyos idrobo, Gael Varoquaux, Jonas Kahn and  Bertrand Thirion\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.externals.joblib import Parallel, delayed, Memory\n",
    "from sklearn.externals import six\n",
    "from scipy.sparse import csgraph, coo_matrix, dia_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "def _compute_weights(masker, data_matrix):\n",
    "    \"\"\"Measuring the Euclidean distance: computer the weights in the direction\n",
    "    of each axis\n",
    "    Note: Here we are assuming a square lattice (no diagonal connections)\n",
    "    \"\"\"\n",
    "    # data_graph shape\n",
    "    dims = len(masker.mask_img_.shape)\n",
    "    data_graph = masker.inverse_transform(data_matrix).get_data()\n",
    "    weights = []\n",
    "\n",
    "    for axis in range(dims):\n",
    "        weights.append(\n",
    "            np.sum(np.diff(data_graph, axis=axis) ** 2, axis=-1).ravel())\n",
    "\n",
    "    return np.hstack(weights)\n",
    "\n",
    "\n",
    "def _compute_edges(data_graph, is_mask=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    dims = len(data_graph.shape)\n",
    "    edges = []\n",
    "    for axis in range(dims):\n",
    "        vertices_axis = np.swapaxes(data_graph, 0, axis)\n",
    "\n",
    "        if is_mask:\n",
    "            edges.append(np.logical_and(\n",
    "                vertices_axis[:-1].swapaxes(axis, 0).ravel(),\n",
    "                vertices_axis[1:].swapaxes(axis, 0).ravel()))\n",
    "        else:\n",
    "            edges.append(np.vstack(\n",
    "                [vertices_axis[:-1].swapaxes(axis, 0).ravel(),\n",
    "                 vertices_axis[1:].swapaxes(axis, 0).ravel()]))\n",
    "    edges = np.hstack(edges)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def _create_ordered_edges(masker, data_matrix):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mask = masker.mask_img_.get_data()\n",
    "    shape = mask.shape\n",
    "    n_features = np.prod(shape)\n",
    "\n",
    "    vertices = np.arange(n_features).reshape(shape)\n",
    "    weights = _compute_weights(masker, data_matrix)\n",
    "    edges = _compute_edges(vertices, is_mask=False)\n",
    "    edges_mask = _compute_edges(mask, is_mask=True)\n",
    "\n",
    "    # Apply the mask\n",
    "    weights = weights[edges_mask]\n",
    "    edges = edges[:, edges_mask]\n",
    "\n",
    "    # Reorder the indices of the graph\n",
    "    max_index = edges.max()\n",
    "    order = np.searchsorted(np.unique(edges.ravel()), np.arange(max_index + 1))\n",
    "    edges = order[edges]\n",
    "\n",
    "    return edges, weights, edges_mask\n",
    "\n",
    "\n",
    "def weighted_connectivity_graph(masker, data_matrix):\n",
    "    \"\"\" Creating weighted graph\n",
    "    data and topology, encoded by a connectivity matrix\n",
    "    \"\"\"\n",
    "    n_features = masker.mask_img_.get_data().sum()\n",
    "\n",
    "    edges, weight, edges_mask = _create_ordered_edges(masker, data_matrix)\n",
    "    connectivity = coo_matrix(\n",
    "        (weight, edges), (n_features, n_features)).tocsr()\n",
    "\n",
    "    # Making it symmetrical\n",
    "    connectivity = (connectivity + connectivity.T) / 2\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "def _nn_connectivity(connectivity, thr):\n",
    "    \"\"\" Fast implementation of nearest neighbor connectivity\n",
    "    connectivity: weighted connectivity matrix\n",
    "    \"\"\"\n",
    "    n_features = connectivity.shape[0]\n",
    "\n",
    "    connectivity_ = coo_matrix(\n",
    "        (1. / connectivity.data, connectivity.nonzero()),\n",
    "        (n_features, n_features)).tocsr()\n",
    "\n",
    "    inv_max = dia_matrix((1. / connectivity_.max(axis=0).toarray()[0], 0),\n",
    "                         shape=(n_features, n_features))\n",
    "\n",
    "    connectivity_ = inv_max * connectivity_\n",
    "\n",
    "    # Dealing with eccentricities\n",
    "    edge_mask = connectivity_.data > 1 - thr\n",
    "\n",
    "    j_idx = connectivity_.nonzero()[1][edge_mask]\n",
    "    i_idx = connectivity_.nonzero()[0][edge_mask]\n",
    "\n",
    "    weight = np.ones_like(j_idx)\n",
    "    edges = np.array((i_idx, j_idx))\n",
    "\n",
    "    nn_connectivity = coo_matrix((weight, edges), (n_features, n_features))\n",
    "\n",
    "    return nn_connectivity\n",
    "\n",
    "\n",
    "def reduce_data_and_connectivity(labels, n_labels, connectivity, data_matrix,\n",
    "                                 thr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_features = len(labels)\n",
    "\n",
    "    incidence = coo_matrix(\n",
    "        (np.ones(n_features), (labels, np.arange(n_features))),\n",
    "        shape=(n_labels, n_features), dtype=np.float32).tocsc()\n",
    "\n",
    "    inv_sum_col = dia_matrix(\n",
    "        (np.array(1. / incidence.sum(axis=1)).squeeze(), 0),\n",
    "        shape=(n_labels, n_labels))\n",
    "\n",
    "    incidence = inv_sum_col * incidence\n",
    "\n",
    "    # reduced data\n",
    "    reduced_data_matrix = (incidence * data_matrix.T).T\n",
    "    reduced_connectivity = (incidence * connectivity) * incidence.T\n",
    "\n",
    "    reduced_connectivity = reduced_connectivity - dia_matrix(\n",
    "        (reduced_connectivity.diagonal(), 0), shape=(reduced_connectivity.shape))\n",
    "\n",
    "    i_idx, j_idx = reduced_connectivity.nonzero()\n",
    "\n",
    "    data_matrix_ = np.maximum(thr, np.sum(\n",
    "        (reduced_data_matrix[:, i_idx] - reduced_data_matrix[:, j_idx]) ** 2, 0))\n",
    "    reduced_connectivity.data = data_matrix_\n",
    "\n",
    "    return reduced_connectivity, reduced_data_matrix\n",
    "\n",
    "\n",
    "def nearest_neighbor_grouping(connectivity, data_matrix, n_clusters, thr):\n",
    "    \"\"\" Cluster according to nn and reduce the data and connectivity\n",
    "    \"\"\"\n",
    "    # Nearest neighbor conenctivity\n",
    "    nn_connectivity = _nn_connectivity(connectivity, thr)\n",
    "\n",
    "    n_features = connectivity.shape[0]\n",
    "\n",
    "    n_labels = n_features - (nn_connectivity + nn_connectivity.T).nnz / 2\n",
    "\n",
    "    if n_labels < n_clusters:\n",
    "        # cut some links to achieve the desired number of clusters\n",
    "        alpha = n_features - n_clusters\n",
    "\n",
    "        nn_connectivity = nn_connectivity + nn_connectivity.T\n",
    "\n",
    "        edges_ = np.array(nn_connectivity.nonzero())\n",
    "\n",
    "        plop = edges_[0] - edges_[1]\n",
    "\n",
    "        select = np.argsort(plop)[:alpha]\n",
    "\n",
    "        nn_connectivity = coo_matrix(\n",
    "            (np.ones(2 * alpha),\n",
    "             np.hstack((edges_[:, select], edges_[::-1, select]))),\n",
    "            (n_features, n_features))\n",
    "\n",
    "    # Clustering step: getting the connected components of the nn matrix\n",
    "    n_labels, labels = csgraph.connected_components(nn_connectivity)\n",
    "\n",
    "    # Reduction step: reduction by averaging\n",
    "    reduced_connectivity, reduced_data_matrix = reduce_data_and_connectivity(\n",
    "        labels, n_labels, connectivity, data_matrix, thr)\n",
    "\n",
    "    return reduced_connectivity, reduced_data_matrix, labels\n",
    "\n",
    "\n",
    "def recursive_nearest_agglomeration(masker, data_matrix, n_clusters, n_iter,\n",
    "                                    thr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Weighted connectivity matrix\n",
    "    connectivity = weighted_connectivity_graph(masker, data_matrix)\n",
    "\n",
    "    # Initialization\n",
    "    labels = np.arange(connectivity.shape[0])\n",
    "    n_labels = connectivity.shape[0]\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        connectivity, data_matrix, reduced_labels = nearest_neighbor_grouping(\n",
    "            connectivity, data_matrix, n_clusters, thr)\n",
    "\n",
    "        labels = reduced_labels[labels]\n",
    "        n_labels = connectivity.shape[0]\n",
    "\n",
    "        if n_labels <= n_clusters:\n",
    "            break\n",
    "\n",
    "    return n_labels, labels\n",
    "\n",
    "\n",
    "\n",
    "class ReNA(BaseEstimator):\n",
    "    \"\"\"\n",
    "    ReNA is useful.\n",
    "    Parameters\n",
    "    ----------\n",
    "    masker: dd\n",
    "    n_cluster: int, optional (default 2)\n",
    "        Number of clusters.\n",
    "    connectivity\n",
    "    scaling: bool, optional (default False)\n",
    "    memory: instance of joblib.Memory or string\n",
    "        Used to cache the masking process.\n",
    "        By default, no caching is done. If a string is given, it is the\n",
    "        path to the caching directory.\n",
    "    n_iter: int, optional (default 10)\n",
    "        Number of iterations of the recursive nearest agglomeration\n",
    "    n_jobs: int, optional (default 1)\n",
    "        Number of jobs in solving the sub-problems.\n",
    "    thr: float in the opened interval (0., 1.), optional (default 1e-7)\n",
    "        Threshold used to deal with eccentricities.\n",
    "    Attributes\n",
    "    ----------\n",
    "    `labels_`: numpy array\n",
    "    `n_clusters_`: int\n",
    "        Number of clusters\n",
    "    `sizes_`: numpy array\n",
    "        It contains the size of each cluster\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=2, connectivity=None, masker=None, memory=None,\n",
    "                 scaling=False, n_iter=10, thr=1e-7, n_jobs=1):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.connectivity = connectivity\n",
    "        self.memory = memory\n",
    "        self.scaling = scaling\n",
    "        self.n_iter = n_iter\n",
    "        self.n_jobs = n_jobs\n",
    "        self.masker = masker\n",
    "        self.thr = thr\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute clustering of the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2D array\n",
    "        \"\"\"\n",
    "\n",
    "        X = check_array(X, ensure_min_features=2)\n",
    "\n",
    "        memory = self.memory\n",
    "        if isinstance(memory, six.string_types):\n",
    "            memory = Memory(cachedir=memory, verbose=0)\n",
    "\n",
    "        if self.n_clusters <= 0:\n",
    "            raise ValueError(\"n_clusters should be an integer greater than 0.\"\n",
    "                             \" %s was provided.\" % str(self.n_clusters))\n",
    "\n",
    "        n_labels, labels = recursive_nearest_agglomeration(\n",
    "            self.masker, X, self.n_clusters, n_iter=self.n_iter, thr=self.thr)\n",
    "\n",
    "        sizes = np.bincount(labels)\n",
    "        sizes = sizes[sizes > 0]\n",
    "\n",
    "        self.labels_ = labels\n",
    "        self.n_clusters_ = np.unique(self.labels_).shape[0]\n",
    "        self.sizes_ = sizes\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply clustering, reduce the dimensionality of the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2D array\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        check_is_fitted(self, 'labels_')\n",
    "        #unique_labels = np.unique(self.labels_)\n",
    "\n",
    "#         nX = []\n",
    "#         for l in unique_labels:\n",
    "#             nX.append(np.mean(X[:, self.labels_ == l], axis=1))\n",
    "#         Xred =  np.array(nX).T\n",
    "        Xred = np.array([np.bincount(self.labels_, X[i,:])/self.sizes_ for i in range(N)])\n",
    "\n",
    "        if self.scaling:\n",
    "            Xred = Xred * np.sqrt(self.sizes_)\n",
    "\n",
    "        return Xred\n",
    "    \n",
    "#     def generate_phi(self)\n",
    "#         phi_T = cluster.transform( np.eye(self.n_features))\n",
    "#         phi = phi_T.T # do sparse\n",
    "#         self.phi = phi\n",
    "#         return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit to data, then perform the clustering (transformation)\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, Xred):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'labels_')\n",
    "\n",
    "        _, inverse = np.unique(self.labels_, return_inverse=True)\n",
    "\n",
    "        if self.scaling:\n",
    "            Xred = Xred / np.sqrt(self.sizes_)\n",
    "        return Xred[..., inverse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYKWbgzcNr9T"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/code-Eng/MRI_ReNA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B42RbOD3P84J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus_MRI_ReNA_colab_n.ipynb  ReNA\t\t      test.py\r\n",
      "MSE_MRI_ReNA.py\t\t      rena_mse_3subjects.png\r\n"
     ]
    }
   ],
   "source": [
    "# used to delete old files in colab\n",
    "#!ls\n",
    "#!rm -rf sample_data\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVIGu_GiUECc"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import nilearn as nl\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "\n",
    "def read_data(study_dir, nsub, psize, npatch_perslice):\n",
    "    \"\"\"\n",
    "    study_dir      : is the study directory where each subjects have different\n",
    "                    type of MRI stored.\n",
    "    nsub           : is indicating the number of subject we have in\n",
    "                    the study file\n",
    "    psize          : is the patch size\n",
    "    npatch_perslice: is number of patches per scan slice\n",
    "    \"\"\"\n",
    "    dirlist = glob.glob(study_dir + '/TBI*')\n",
    "    subno = 0\n",
    "    patch_data = np.zeros((0, 0, 0, 0))\n",
    "    for subj in dirlist:\n",
    "\n",
    "        t1file = os.path.join(subj, 'T1mni.nii.gz')\n",
    "        t2file = os.path.join(subj, 'T2mni.nii.gz')\n",
    "        fl = os.path.join(subj, 'FLAIRmni.nii.gz')\n",
    "        # check if three MRI contrast exist if not skip subject\n",
    "        if not (os.path.isfile(t1file) and os.path.isfile(t2file)\n",
    "                and os.path.isfile(fl)):\n",
    "            print(\"one file missing (T1,T2 or FLAIR), go to the next subject\")\n",
    "            subno += 1\n",
    "            continue\n",
    "\n",
    "        if subno < nsub:\n",
    "            subno = subno + 1\n",
    "            print(\"subject %d \" % (subno))\n",
    "        else:\n",
    "            break\n",
    "        # Read the three images\n",
    "        t1 = nl.image.load_img(t1file).get_data()\n",
    "        t2 = nl.image.load_img(t2file).get_data()\n",
    "        flair = nl.image.load_img(fl).get_data()\n",
    "\n",
    "        p = np.percentile(np.ravel(t1), 95)  #normalize to 95 percentile\n",
    "        t1 = np.float32(t1) / p\n",
    "\n",
    "        p = np.percentile(np.ravel(t2), 95)  #normalize to 95 percentile\n",
    "        t2 = np.float32(t2) / p\n",
    "\n",
    "        p = np.percentile(np.ravel(flair), 95)  #normalize to 95 percentile\n",
    "        flair = np.float32(flair) / p\n",
    "\n",
    "        imgs = np.stack((t1, t2, flair), axis=3)\n",
    "\n",
    "        for sliceno in range(imgs.shape[2]):\n",
    "            ptch = extract_patches_2d(\n",
    "                image=imgs[:, :, sliceno, :],\n",
    "                patch_size=psize,\n",
    "                max_patches=npatch_perslice)\n",
    "            if patch_data.shape[0] == 0:\n",
    "                patch_data = ptch\n",
    "            else:\n",
    "                patch_data = np.concatenate((patch_data, ptch), axis=0)\n",
    "\n",
    "        # Read coronal slices\n",
    "\n",
    "        #        create image\n",
    "\n",
    "        #        create random patches\n",
    "\n",
    "    return patch_data # npatch x width x height x channels\n",
    "\n",
    "\n",
    "def get_single_subject(file_name):\n",
    "\n",
    "    t1file = os.path.join(file_name, 'T1.nii.gz')\n",
    "    t2file = os.path.join(file_name, 'T2.nii.gz')\n",
    "    fl = os.path.join(file_name, 'FLAIR.nii.gz')\n",
    "\n",
    "    t1 = nifti_masker.transform(image.load_img(t1file))\n",
    "    t2 = nifti_masker.transform(image.load_img(t2file))\n",
    "    flair = nifti_masker.transform(image.load_img(fl))\n",
    "\n",
    "    p = np.percentile(np.ravel(t1), 95)  # normalize to 95 percentile\n",
    "    t1 = np.float32(t1) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(t2), 95)  # normalize to 95 percentile\n",
    "    t2 = np.float32(t2) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(flair), 95)  # normalize to 95 percentile\n",
    "    flair = np.float32(flair) / p\n",
    "\n",
    "    imgs = np.concatenate((t1, t2, flair))\n",
    "\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6MJTa41Ww7S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: invalid value encountered in true_divide\n",
      "/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-727881504661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                  \u001b[0mnsub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                  \u001b[0mpsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                  npatch_perslice=32)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0a55312df3e2>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(study_dir, nsub, psize, npatch_perslice)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msliceno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 max_patches=npatch_perslice)\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatch_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mpatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/image.py\u001b[0m in \u001b[0;36mextract_patches_2d\u001b[0;34m(image, patch_size, max_patches, random_state)\u001b[0m\n\u001b[1;32m    371\u001b[0m                          \" of the image.\")\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0mn_colors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 568\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/big_disk/ajoshi/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# read all data in patches \n",
    "\n",
    "\n",
    "data_dir = '/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/'\n",
    "\n",
    "# Read data\n",
    "data = read_data(study_dir=data_dir,\n",
    "                 nsub=5,\n",
    "                 psize=[128,128],\n",
    "                 npatch_perslice=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjeA4_AnnbKi"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0i3qWBFRf-oq"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "imgg = nib.load('/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/TBI_INVAX364NTZ/T1.nii.gz')\n",
    "type(imgg)\n",
    "print(imgg.header.get_data_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pJoWH4tFZLJ"
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "plotting.plot_stat_map(stat_map_img='/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/TBI_INVAX364NTZ/T1.nii.gz',\n",
    "                       cut_coords=(-30, -65, -10),\n",
    "                       threshold=10e-7, \n",
    "                       title=\"dim=-.5\",\n",
    "                       dim=-.5,\n",
    "                       black_bg='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxAHg-e7Xius"
   },
   "source": [
    "# ReNA \n",
    "### reference - [GitHub](https://github.com/ahoyosid/ReNA/blob/master/Example_Neuroimaging.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "1.   Loading Data\n",
    "2.   Get the connectivity (spatial structure)\n",
    "3.   Visualizing the result \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oy7dMdR12u1X"
   },
   "source": [
    "#MSE MRI Adding New subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aueUUADmxX1g"
   },
   "source": [
    "## Plot MSE for each images per subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3689
    },
    "colab_type": "code",
    "id": "75K0I7ds3AU5",
    "outputId": "83db6920-763f-40f6-b095-7bde0b9d14f4"
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/big_disk/ajoshi/coding_ground/lesion-detector/src/sergul/ReNA')\n",
    "import nilearn \n",
    "from nilearn import image\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import nibabel as nb\n",
    "import os\n",
    "import numpy as np\n",
    "import rena\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Befor run edit the dir_name to the path of your subject; include / at the end\n",
    "dir_name = \"/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/\"\n",
    "f_len = len(dir_name.split('/'))\n",
    "f_len = f_len-2\n",
    "\n",
    "'''mask_img = nb.load('/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/TBI_INVAX364NTZ/T1.nii.gz')\n",
    "nifti_masker = NiftiMasker(mask_img=mask_img, smoothing_fwhm=False,\n",
    "                           standardize=False)\n",
    "nifti_masker.fit()'''\n",
    "\n",
    "'''# using subject TBI_INVDD132CG0 FLAIR as masker for all data\n",
    "nifti_masker = NiftiMasker( smoothing_fwhm=False,\n",
    "                           standardize=False)\n",
    "nifti_masker.fit('MRI_ReNA/supporting_data/grey10_icbm_2mm_bin.nii.gz')'''\n",
    "\n",
    "\n",
    "mask_img = nb.load('/big_disk/ajoshi/coding_ground/lesion-detector/supporting_data/grey10_icbm_2mm_bin.nii.gz')\n",
    "nifti_masker = NiftiMasker(mask_img=mask_img, smoothing_fwhm=False,\n",
    "                           standardize=False)\n",
    "nifti_masker.fit()\n",
    "\n",
    "def subject_checker(study_folder):\n",
    "    \n",
    "    subject_list = glob.glob(study_folder + 'TBI*')\n",
    "    target_subject = []\n",
    "\n",
    "    for subject in subject_list:\n",
    "      \n",
    "      t1file = os.path.join(subject,'T1.nii.gz')\n",
    "      t2file = os.path.join(subject,'T2.nii.gz')\n",
    "      fl = os.path.join(subject,'FLAIR.nii.gz')\n",
    "\n",
    "      if not (os.path.isfile(t1file) and os.path.isfile(t2file) \n",
    "              and os.path.isfile(fl)):\n",
    "          print(\"one file missing (T1,T2 or FLAIR), go to the next subject\\n\")\n",
    "          continue \n",
    "      else:\n",
    "        target_subject.append(subject)\n",
    "    \n",
    "    return target_subject\n",
    "      \n",
    "def get_single_subject(file_name):\n",
    "  \n",
    "    t1file = os.path.join(file_name, 'T1.nii.gz')\n",
    "    t2file = os.path.join(file_name, 'T2.nii.gz')\n",
    "    fl = os.path.join(file_name, 'FLAIR.nii.gz')\n",
    "\n",
    "    t1 = nifti_masker.transform(image.load_img(t1file))\n",
    "    t2 = nifti_masker.transform(image.load_img(t2file))\n",
    "    flair = nifti_masker.transform(image.load_img(fl))\n",
    "\n",
    "    p = np.percentile(np.ravel(t1), 95)  # normalize to 95 percentile\n",
    "    t1 = np.float32(t1) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(t2), 95)  # normalize to 95 percentile\n",
    "    t2 = np.float32(t2) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(flair), 95)  # normalize to 95 percentile\n",
    "    flair = np.float32(flair) / p\n",
    "\n",
    "    imgs = np.concatenate((t1, t2, flair))\n",
    "\n",
    "    return imgs\n",
    "  \n",
    "\n",
    "all_imgs = None\n",
    "\n",
    "# checker include only the subject with T1,T2 and FLAIR MRI data\n",
    "subject_list = subject_checker(dir_name)\n",
    "#print(subject_list) # just to chekc we have the right subject \n",
    "\n",
    "for subject_name in subject_list:\n",
    "    \n",
    "    file_name = subject_name\n",
    "    imgs = get_single_subject(file_name)\n",
    "    \n",
    "    if all_imgs is None:\n",
    "        all_imgs = imgs\n",
    "    else:\n",
    "        all_imgs = np.concatenate((all_imgs, imgs))\n",
    "\n",
    "      \n",
    "print('all images concatenate shape is ',all_imgs.shape,'\\n')\n",
    "\n",
    "n_voxels = all_imgs.shape[1]\n",
    "n_samples = all_imgs.shape[0]\n",
    "n_clusters = int(20*n_voxels/100)\n",
    "cluster = rena.ReNA(scaling=True,\n",
    "               n_clusters=n_clusters,\n",
    "               masker=nifti_masker)\n",
    "\n",
    "print(' no. of voxels: ',n_voxels,'\\n',\n",
    "      'no. of samples: ',n_samples,'\\n',\n",
    "      'no. of clusters: ',n_clusters,'\\n\\n')\n",
    "\n",
    "cluster.fit(all_imgs)\n",
    "\n",
    "reduced_images = cluster.transform(all_imgs)\n",
    "reconstructed_images = cluster.inverse_transform(reduced_images)\n",
    "\n",
    "mse = np.mean(abs(all_imgs - reconstructed_images)**2, axis=1)\n",
    "a_relative_chg = np.array(abs(all_imgs - reconstructed_images)/abs(all_imgs))\n",
    "se = np.array(abs(all_imgs - reconstructed_images)**2)\n",
    "\n",
    "labels_plot = []\n",
    "for labels_p in subject_list:\n",
    "    c_list = glob.glob(labels_p + '/*')\n",
    "    # if there are other type nii.gz not wanted included as shown with 'fse.nii.gz'\n",
    "    # ../../sample_data/TBI*/*  >>> 0/1/2/3/4\n",
    "    # use f_len here\n",
    "    for c in c_list:\n",
    "      if c.split('/')[f_len+2] == 'T1.nii.gz':\n",
    "        labels_plot.append(c.split('/')[f_len+1] + '-' + c.split('/')[f_len+2][0:2])\n",
    "    for c in c_list:\n",
    "      if c.split('/')[f_len+2] == 'T2.nii.gz':\n",
    "        labels_plot.append(c.split('/')[f_len+1] + '-' + c.split('/')[f_len+2][0:2])\n",
    "    for c in c_list:\n",
    "      if c.split('/')[f_len+2] == 'FLAIR.nii.gz':\n",
    "        labels_plot.append(c.split('/')[f_len+1] + '-' + c.split('/')[f_len+2][0:2])\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(mse, marker='o')\n",
    "ax.set_xticks(range(n_samples))\n",
    "ax.set_xticklabels(labels_plot, rotation=90)\n",
    "ax.set_ylabel(\"MSE\")\n",
    "plt.tight_layout()\n",
    "\n",
    "df1 = pd.Series(labels_plot)\n",
    "df2 = pd.Series(mse)\n",
    "df = pd.DataFrame(df1,columns=['Subject'])\n",
    "df['MSE'] = df2\n",
    "df = df.sort_values(by='MSE',ascending=False)\n",
    "dfplot = df.plot(x='Subject', y='MSE',marker='o',color='g')\n",
    "dfplot.set_xticks(range(n_samples))\n",
    "dfplot.set_xticklabels(df['Subject'],rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "fAwupsQ3-DcY",
    "outputId": "b8ae07d8-1941-4a55-ec48-a844eea22dbf"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAtSgSKhU1Ag"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnWwz-59xCwy"
   },
   "source": [
    "## Average MSE for three images per subject (required to run previous code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "colab_type": "code",
    "id": "jKCK94aR2__l",
    "outputId": "db1280f5-4834-42d2-92ba-d351d9aa2d3d"
   },
   "outputs": [],
   "source": [
    "labels_plot2 = []\n",
    "for labels_p in subject_list:\n",
    "    labels_plot2.append(labels_p.split('/')[f_len+1])\n",
    "\n",
    "#labels_plot2 = np.array(labels_plot2)\n",
    "mse1 = np.array([(sum(mse[i:i+3]))/3 for i in range(0,len(mse),3)])\n",
    "nn_samples = int(n_samples/3)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(mse1, marker='o')\n",
    "ax.set_xticks(range(nn_samples))\n",
    "ax.set_xticklabels(labels_plot2, rotation=90)\n",
    "ax.set_ylabel(\"Average MSE for three images per subject\")\n",
    "plt.tight_layout()\n",
    "\n",
    "df1 = pd.Series(labels_plot2)\n",
    "df2 = pd.Series(mse1)\n",
    "df = pd.DataFrame(df1,columns=['Subject'])\n",
    "df['MSE'] = df2\n",
    "df = df.sort_values(by='MSE',ascending=False)\n",
    "dfplot2 = df.plot(x='Subject', y='MSE',marker='o',color='g')\n",
    "dfplot2.set_xticks(range(nn_samples))\n",
    "dfplot2.set_xticklabels(df['Subject'],rotation=90)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmGONTWmwnbp"
   },
   "source": [
    "## ReNA on one subject then can plot needed images (do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B2acPsiRwIx2"
   },
   "outputs": [],
   "source": [
    "import nilearn \n",
    "from nilearn import image\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import nibabel as nb\n",
    "import os\n",
    "import numpy as np\n",
    "import rena\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Befor run edit the dir_name to the path of your subject; include / at the end\n",
    "dir_name = \"/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/\"\n",
    "f_len = len(dir_name.split('/'))\n",
    "f_len = f_len-2\n",
    "\n",
    "\n",
    "\n",
    "mask_img = nb.load('/big_disk/ajoshi/coding_ground/lesion-detector/supporting_data/grey10_icbm_2mm_bin.nii.gz')\n",
    "nifti_masker = NiftiMasker(mask_img=mask_img, smoothing_fwhm=False,\n",
    "                           standardize=False)\n",
    "nifti_masker.fit()\n",
    "\n",
    "def subject_checker(study_folder):\n",
    "    \n",
    "    subject_list = glob.glob(study_folder + 'TBI*')\n",
    "    target_subject = []\n",
    "\n",
    "    for subject in subject_list:\n",
    "      \n",
    "      t1file = os.path.join(subject,'T1.nii.gz')\n",
    "      t2file = os.path.join(subject,'T2.nii.gz')\n",
    "      fl = os.path.join(subject,'FLAIR.nii.gz')\n",
    "\n",
    "      if not (os.path.isfile(t1file) and os.path.isfile(t2file) \n",
    "              and os.path.isfile(fl)):\n",
    "          print(\"one file missing (T1,T2 or FLAIR), go to the next subject\\n\")\n",
    "          continue \n",
    "      else:\n",
    "        target_subject.append(subject)\n",
    "    \n",
    "    return target_subject\n",
    "      \n",
    "def get_single_subject(file_name):\n",
    "  \n",
    "    t1file = os.path.join(file_name, 'T1.nii.gz')\n",
    "    t2file = os.path.join(file_name, 'T2.nii.gz')\n",
    "    fl = os.path.join(file_name, 'FLAIR.nii.gz')\n",
    "\n",
    "    t1 = nifti_masker.transform(image.load_img(t1file))\n",
    "    t2 = nifti_masker.transform(image.load_img(t2file))\n",
    "    flair = nifti_masker.transform(image.load_img(fl))\n",
    "\n",
    "    p = np.percentile(np.ravel(t1), 95)  # normalize to 95 percentile\n",
    "    t1 = np.float32(t1) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(t2), 95)  # normalize to 95 percentile\n",
    "    t2 = np.float32(t2) / p\n",
    "\n",
    "    p = np.percentile(np.ravel(flair), 95)  # normalize to 95 percentile\n",
    "    flair = np.float32(flair) / p\n",
    "\n",
    "    imgs = np.concatenate((t1, t2, flair))\n",
    "\n",
    "    return imgs\n",
    "  \n",
    "\n",
    "all_imgs = None\n",
    "\n",
    "# checker include only the subject with T1,T2 and FLAIR MRI data\n",
    "subject_list = ['/big_disk/ajoshi/fitbir/preproc/tracktbi_pilot/TBI_INVWZ015VJB/'] #subject_checker(dir_name)\n",
    "\n",
    "for subject_name in subject_list:\n",
    "    \n",
    "    file_name = subject_name\n",
    "    imgs = get_single_subject(file_name)\n",
    "    \n",
    "    if all_imgs is None:\n",
    "        all_imgs = imgs\n",
    "    else:\n",
    "        all_imgs = np.concatenate((all_imgs, imgs))\n",
    "\n",
    "      \n",
    "print('all images concatenate shape is ',all_imgs.shape,'\\n')\n",
    "\n",
    "n_voxels = all_imgs.shape[1]\n",
    "n_samples = all_imgs.shape[0]\n",
    "n_clusters = int(20*n_voxels/100)\n",
    "cluster = rena.ReNA(scaling=True,\n",
    "               n_clusters=n_clusters,\n",
    "               masker=nifti_masker)\n",
    "\n",
    "print(' no. of voxels: ',n_voxels,'\\n',\n",
    "      'no. of samples: ',n_samples,'\\n',\n",
    "      'no. of clusters: ',n_clusters,'\\n\\n')\n",
    "\n",
    "cluster.fit(all_imgs)\n",
    "\n",
    "reduced_images = cluster.transform(all_imgs)\n",
    "reconstructed_images = cluster.inverse_transform(reduced_images)\n",
    "\n",
    "# apply abs(reconstructed-original)/abs(original) \n",
    "\n",
    "relative_chg = np.array(abs(reconstructed_images - all_imgs)/abs(all_imgs))\n",
    "\n",
    "from nilearn import plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krvZt1Tbw3N8"
   },
   "source": [
    "## Plotting MRI images after running ReNA (require previous code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y0QJcqqpkOeE"
   },
   "source": [
    "To find idex for specific subject use the following:\n",
    "`labels_plot.index('TBI_INVEU482TRG-FL')`\n",
    "the name you use in the method must be similar as the generated one in MSE chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tNpRzuVaknng",
    "outputId": "6a075f7b-dfc9-4f87-d268-b5477a53d925"
   },
   "outputs": [],
   "source": [
    "labels_plot.index('TBI_INVEU482TRG-FL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "UfNbzrLv5n4j",
    "outputId": "19babd1b-a22f-4d07-f7d9-6629a221d8b9"
   },
   "outputs": [],
   "source": [
    "print(reconstructed_images.shape)\n",
    "print(mse.shape)\n",
    "print(a_relative_chg.shape)\n",
    "print(se.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2110
    },
    "colab_type": "code",
    "id": "tw8jB_vzB6hx",
    "outputId": "d021fac1-0fcb-4084-d4c2-80bd644c7b6e"
   },
   "outputs": [],
   "source": [
    "from nilearn import plotting, image\n",
    "# subjeact TBI_INVWZ015VJB [21 T1] \n",
    "# labels_plot.index('TBI_INVWZ015VJB-T1') = 21\n",
    " \n",
    "plotting.plot_roi(nifti_masker.inverse_transform(all_imgs[21]), title=\"roi TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)\n",
    "\n",
    "plotting.plot_epi(nifti_masker.inverse_transform(all_imgs[21]), title=\"epi TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)\n",
    "plotting.plot_epi(nifti_masker.inverse_transform(reconstructed_images[21]), title=\"epi ReNA reconstructed TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)\n",
    "\n",
    "plotting.plot_epi(nifti_masker.inverse_transform(se[21]), title=\"epi squared error TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)\n",
    "\n",
    "plotting.plot_epi(nifti_masker.inverse_transform(a_relative_chg[21]), title=\"epi Relative change TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)\n",
    "                  \n",
    "#Smooth the relative changes\n",
    "mean_func = image.mean_img(nifti_masker.inverse_transform((a_relative_chg[21])))\n",
    "\n",
    "# Then we smooth, with a varying amount of smoothing, from none to 20mm\n",
    "# by increments of 5mm\n",
    "\n",
    "smoothed_img = image.smooth_img(mean_func, 15)\n",
    "plotting.plot_epi(smoothed_img, title=\"epi 15mm smoothing Relative change TBI_INVWZ015VJB T1\",\n",
    "                  cut_coords=(20, 0,0),black_bg='True',colorbar=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "BcUH44W9Yf3n",
    "4KCG5llYVumG",
    "MxAHg-e7Xius",
    "PTFUE4bOs4sk",
    "mr-MXRAaoBFo",
    "sRmNGDRt3wDR",
    "GAWb3olZ3hjn",
    "fMX2Fhea3B4r",
    "4LBISHGZ4pw6",
    "fuLceNrg9qRu",
    "QgnyioPPxiOV",
    "bnWwz-59xCwy",
    "o-6RuEvNwXlQ",
    "mmGONTWmwnbp",
    "euMsTF9iLrf1",
    "UMYDJreOTD8u"
   ],
   "include_colab_link": true,
   "name": "Focus_MRI_ReNA_colab_n.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
